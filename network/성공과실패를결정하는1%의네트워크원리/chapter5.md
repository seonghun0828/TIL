# 서버측의 LAN에는 무엇이 있는가?
[1. 웹 서버의 설치 장소](#1-웹-서버의-설치-장소)   
[2. 방화벽의 원리와 동장](#2-방화벽의-원리와-동작)   
[3. 복수 서버에 리퀘스트를 분배한 서버의 부하 분산](#3-복수-서버에-리퀘스트를-분배한-서버의-부하-분산)   
[4. 캐시 서버를 이용한 서버의 부하 분산](#4-캐시-서버를-이용한-서버의-부하-분산)   
[5. 콘텐츠 배포 서비스](#5-콘텐츠-배포-서비스)   
   
## 이 질문에 대답할 수 없으면 정독하기   
- 현재 주류가 되어 있는 방화벽의 유형을 무엇이라고 하는가?   
- 방화벽에서 애플리케이션의 종류를 지정할 때 점검하는 정보는 무엇인가?   
- 웹 서버의 부하를 분산하기 위해 액세스를 여러 대의 서버에 분배하는 장치를 무엇이라고 하는가?   
- 서버측에 설치하는 것은 포워드 프록시와 리버스 프록시 중 어느 것인가? 그리고 각각을 설명하라.   
- 인터넷에 다수의 캐시 서버를 설치하고, 이것을 웹 서버 운영자에게 대출하는 서비스를 무엇이라하는가? 그 예시는?
   
## 1 웹 서버의 설치 장소
    1. 사내에 웹 서버를 설치하는 경우   
현재 이 방법은 주류에서 밀려났다. IP 주소의 부족과 보안상의 이유 때문이다.   
지금은 사내 LAN과 서버를 분리해 방화벽을 두는 방법이 일반화되었다.   

    2. 데이터센터에 웹 서버를 설치하는 경우   
데이터센터라는 시설에 서버를 가지고 들어가거나 프로바이더 등이 소유하는 서버를 빌려쓴다.   
프로바이더들은 인터넷의 중심 부분에 고속 회선으로 접속되었으므로 고속으로 액세스가 가능하다.   
또 데이터센터는 내진 구조, 자가 발전, 24시간 관리 등 회사 안보다 안전성이 높다.   

## 2 방화벽의 원리와 동작
    1. 패킷 필터링형이 주류이다   
서버의 설치 장소와 관계 없이 지금은 바로 앞에 방화벽이 보통 존재한다.   
방화벽은 특정 서버와 해당 서버 안의 특정 애플리케이션에 액세스하는 패킷만 통과시키고 그 외 패킷은 차단한다.   
네트워크에는 다양한 패킷이 흐르고 있으므로 어떤 패킷을 통과 혹은 차단할지 선별하기 위해 다양한 방법이 고안되었다.   
현재 가장 많이 보급된 방법은 패킷 필터링형 방화벽이다.   

    2. 패킷 필터링의 조건 설정 개념   
패킷의 헤더에는 통신 동작을 제어하는 제어 정보가 들어있다. 그중 패킷 필터링의 조건 설정에서 자주 사용되는 것은 다음과 같다.   
1) MAC 헤더(송신처 MAC 주소), 2) IP 헤더(송신처 IP 주소, 수신처 IP 주소, 프로토콜 번호),   
3) TCP 헤더 또는 UDP 헤더(송신처 포트 번호, 수신처 포트 번호, TCP 컨트롤 비트, 프래그먼트), 4) 헤더가 아니라 ICMP 메시지의 내용   
   
1-1) 송신처 MAC 주소 : 라우터는 패킷을 중계할 때 MAC 주소를 바꿔쓰고 중계 대상 라우터의 MAC 주소는 수신처 MAC 주소 항목에,   
    자신의 MAC 주소는 송신처 MAC 주소 항목에 기록한다. 송신처 MAC 주소를 조사해 직전에 중계한 라우터 MAC 주소를 알 수 있다.   
    
2-3) 프로토콜 번호 : TCP/IP 프로토콜은 종류마다 번호가 할당되어 있다. IP : 0 / ICMP : 1 / TCP : 6 / UDP : 17 / OSPF : 89    
   
3-1) 송신처 포트 번호 : 패킷을 송신한 프로그램에 할당된 포트 번호. 서버 프로그램에 할당된 포트 번호는 고정되어 있으므로,   
    서버에서 반송된 패킷의 포트 번호에서 서버 프로그램을 판별할 수 있다. 수신의 경우도 마찬가지다.   
    그러나 클라이언트측 프로그램의 포트 번호는 미사용 번호가 무작위로 할당되므로 이것으로 무언가를 판단하기는 어렵다.   
    따라서 클라이언측에서 송신 혹은 수신된 포트 번호를 조건으로 설정하는 경우는 거의 없다.   
    
3-3) TCP 컨트롤 비트 : TCP 프로토콜의 제어에 사용되는 정보. 주로 접속에 관한 제어에 사용한다.   
ACK : 수신 데이터의 일련번호 필드가 유효한 것을 나타냄. 데이터 수신 완료를 송신측에 알릴 때 사용.   
PSH : 송신측의 애플리케이션 프로그램이 송신 버퍼에 데이터를 저장하지 않고 곧바로 송신하도록 지시.   
RST : 접속을 강제로 종료하고, 이상 종료시에 사용.   
SYN : 통신을 개시할 때 접속 동작에서 최초로 보낸 패킷은 SYN 비트가 1, ACK 비트가 0이 된다.   
FIN : 연결 끊기   
   
3-4) 프래그먼트 : IP 프로토콜의 조각 나누기 기능으로, 패킷을 분할하고 이 패킷이 두 번째 이후임을 나타낸다.       
   
4-1) ICMP 메시지의 유형 : ICMP 메시지는 패킷 배송 도중 이상이 발생했음을 알리거나 통신 상대의 동작을 확인할 때 사용.   
0 : ping 명령으로 보내는 ICMP 에코 메시지에 응답하는 것.   
8 : ICMP 에코라고 한다. ping 명령을 실행하면 ICMP 에코 메시지가 송신됨.   
   
패킷 필터링의 조건을 설정할 때는 먼저 패킷의 흐름에 착안한다. 수신처 IP 주소와 송신처 IP 주소에 따라 시점과 종점을 판단한다.   
인터넷에서 웹 서버로 흐르는 패킷은 보낼 때의 시점은 지정할 수 없지만, 종점은 웹 서버가 되므로 이것을 조건으로 설정할 수 있다.   
송신처 IP 주소에 따라 시점을 지정할 수 있으면 이것도 조건에 추가한다.   
   
서버에서 인터넷으로 액세스하는 것은 부정 소프트웨어의 감염을 막기 위해 기본적으로 금지하고 있다.   
그러나 패킷을 받으면 정확히 도착했는지를 송신측에 알리는 수신 확인 응답 구조가 작용하므로 웹 서버에서 인터넷으로 흐르는 패킷도 있다.   
이 패킷도 웹 서버에서 인터넷측으로 흘러간 후 그곳에서 시점(송신처 IP 주소)이 웹 서버의 주소와 일치하는 패킷은 통과한다.
   
기본적으로 수신처나 송신처의 주소가 패킷을 통과시킬 것인지 차단할 것인지를 결정하는 첫걸음인 것이다.   

    3. 애플리케이션을 한정할 때 포트 번호를 사용한다   
보안에서는 어떤 패킷이 어떤 위험성이 잠재하는지 모르기 때문에 불필요한 것은 전부 차단하고 필요한 것만 허용하는 방법이 좋다.   
지금처럼 웹을 사용하고 있을 때는 웹 애플리케이션으로 한정하는 것이 좋은 것이다.   
   
특정 애플리케이션으로 조건을 한정할 때는 TCP 헤더나 UDP 헤더에 기록되어 있는 포트 번호를 조건으로 추가한다.   
웹 서버의 포트 번호는 80번으로 결정되어 있으므로 전술한 수신처 IP 주소 및 송신처 IP 주소에 수신처 포트 번호가 80번인 경우의 조건도 추가한다.   
웹 서버 이외의 애플리케이션에 대한 액세스를 허가할 경우에는 이 애플리케이션의 포트 번호를 설정해 이것이 통과하도록 한다.   

    4. 컨트롤 비트로 접속 방향을 판단한다   
웹의 동작은 TCP 프로토콜을 사용해 양방향으로 패킷이 흐른다.    
그러므로 단순히 웹 서버에서 인터넷으로 흐르는 패킷을 정지시키면 인터넷에서 웹 서버에 액세스하는 동작도 정지되는 것이다.   
패킷이 흐르는 방향이 아니라 액세스 방향을 판단해 정지시켜야 하는데, 여기에서 TCP 헤더의 컨트롤 비트를 활용할 수 있다.   
   
TCP는 최초에 행하는 접속 단계의 동작에서 3개의 패킷이 흐르는데, 최초의 패킷만 TCP 컨트롤 비트의    
SYN이라는 비트가 1이 되고, ACK라는 비트가 0이 된다. 이 값을 조사해 최초의 패킷과 그 이후 패킷을 판별할 수 있다.   
   
최초의 패킷이 웹 서버측에서 인터넷측으로 흘러갈 경우 이것을 차단하도록 설정한다.   
이것을 차단하면 상대로부터 두 번째 패킷이 돌아오는 경우가 없으므로 당연히 TCP의 접속 동작은 실패로 끝난다.   
웹 서버가 기점이 되어 인터넷에 액세스하려고 해도 접속 동작이 틀림 없이 실패한다는 것이다.   
   
인터넷측에서 웹 서버에 액세스할 때는 최초의 패킷은 수신처가 웹 서버를 나타내고 컨트롤 비트가 SYN=1, ACK=0이므로 통과한다.   
두 번째 패킷은 송신처가 웹 서버를 나타내지만, 컨트롤 비트가 SYN=0, ACK=1이므로 통과한다.   
결국 인터넷측에서 웹 서버에 액세스할 때 흐르는 패킷은 전부 패킷 필터링을 통과하는 것이다.   
   
실제로는 통과시키는 것과 차단하는 것을 완전히 선별할 수 없는 경우도 있는데, DNS 서버에 대한 액세스가 대표적인 예다.   
DNS 서버에 조회하는 동작은 UDP를 사용하는데, UDP는 접속 단계의 동작이 없으므로 액세스 방향을 판별할 수 없다.   
이 성질은 UDP를 사용하는 애플리케이션에 공통이다. 이 경우에는 패킷을 전부 통과시키거나, 전면 차단하는 방법을 선택해야 한다.   

    5. 사내 LAN에서 공개 서버용 LAN으로 조건을 설정한다   
인터넷과 공개 서버용 LAN(서버)을 왕래하는 패킷의 조건을 설정할 뿐 아니라    
사내 LAN과 인터넷 또는 사내 LAN과 공개 서버용 LAN을 왕래하는 패킷의 조건도 설정해야 합니다.   

    6. 밖에서 사내 LAN으로 액세스할 수 없다   
패킷 필터링형 방화벽은 패킷을 통과시킬지, 차단시킬지를 판단할 뿐만 아니라 주소 변환의 기능도 가지고 있다.   
인터넷과 사내 LAN을 왕래하는 패킷은 주소 변환을 해야 하므로 설정이 필요합니다.   
주소 변환을 이용하면 인터넷측에서 사내 LAN에는 액세스할 수 없게 된다.   

    7. 방화벽을 통과한다   
방화벽에 의해 패킷이 차단되면 패킷을 버리고, 버린 기록은 남긴다. 부정 침입 흔적을 분석해 향후 활용하기 위해서다.   
통과시킨다는 판정을 내린 경우 패킷을 중계하는데, 이 중계 동작은 라우터의 동작과 같다.   
복잡한 조건 설정이나 버린 패킷의 기록이 필요하지 않으면 전용 하드웨어나 소프트웨어를 사용하지 않고 패킷 필터링 기능을 가진 라우터를 방화벽으로 사용할 수 있다.   

    8. 방화벽으로 막을 수 없는 공격
방화벽은 시점과 종점만 조사하므로 패킷의 내용을 조사하지 않으면 위험한지 판단할 수 없어 방화벽의 구조는 이런 상황에 대처할 수 없다.   
두 가지 대처법이 있는데, 웹 서버를 항상 새로운 버전으로 갱신해 보안을 유지하는 것이다.   
또 하나는 방화벽과는 별도로 패킷의 내용을 조사해 위험한 데이터가 포함된 경우 패킷을 차단하는 것이다.

## 3 복수 서버에 리퀘스트를 분배한 서버의 부하 분산
    1. 처리 능력이 부족하면 복수 서버로 부하 분산된다   
대량의 패킷에 서버의 처리 능력이 부족해지면 서버 머신을 고성능으로 교체하는 것이 가장 먼저 떠오르는 방법이다.   
이때 다수의 사용자가 집중적으로 액세스하면 아무리 고성능 기종이라 할 지라도 서버 한대로는 따라잡지 못할 것이다.   
그럴 때 복수의 서버를 사용해 처리를 분담하는 분산 처리 방법을 사용하고, 분산 처리에는 여러 방식이 있다.   
   
가장 간단한 방법은 여러 대의 웹 서버를 설치하고 한 대가 담당하는 사용자 수를 줄이는 방법이다.   
이 방법을 취할 경우 클라이언트가 보내는 리퀘스트를 웹 서버에 분배하는 구조가 필요하다.   
구체적인 방법 중 DNS 서버에서 분배하는 방법이 가장 간단하다.   
   
서버에 액세스할 때 DNS 서버에 조회하여 IP 주소를 조사하는데, DNS 서버에 같은 이름으로 여러 대의 웹 서버를 등록해 놓는다.   
그러면 DNS 서버는 조회가 있을 때마다 차례대로 IP 주소를 되돌려주는 것이다.   
   
www.github.com 이라는 서버명에 A, B, C라는 3개의 IP 주소를 대응시킨다고 가정해본다.   
최초의 조회에는 A B C 라고 3개를 나란히 회답하고, 다음 조회에는 B, C, A를, 다음에는 C, A, B를 회답한다는 식이다.   
그리고 1주기를 순환하고 원래대로 돌아가는데 이 방법을 ***라운드 로빈***이라고 한다.   
   
이 방법에는 결점이 있는데, 웹 서버가 많으면 이 중 고장나는 것도 있을 것이다. 이때 고장난 웹 서버를 피해서   
IP 주소를 회답하면 좋지만, 보통 DNS 서버는 웹 서버의 동작 유무를 확인하지 못하므로 웹 서버가 정지해도 상관 않고 IP 주소를 회답해 버린다.   
   
라운드 로빈에서 차례대로 웹 서버를 분배하면 좋지 않은 상태가 생길 수 있다.   
CGI 등의 애플리케이션에서 페이지를 동적으로 만드는 경우 복수의 페이지에 걸쳐 대화할 수 있는데, 웹 페이지가 변하면 대화가 도중에 끊길 수 있기 때문이다.   
쇼핑 사이트에서 첫 번째 페이지에 주소나 이름을 입력하고, 다음 페이지에서 신용카드 번호를 입력하는 것과 같은 경우가 있다.   

    2. 부하 분산 장치를 이용해 복수의 웹 서버로 분할된다   
이런 좋지 않은 상태를 피하기 위해 ***부하 분산 장치*** 또는 ***로드 밸런서*** 등으로 부르는 기기가 고안되었다.   
먼저 부하 분산 장치를 웹 서버 대신 DNS 서버에 등록한다. 그러면 클라이언트는 여기에 리퀘스트 메시지를 전송한다.   
그러면 어느 웹 서버에 리퀘스트를 전송해야 할지 판단해야 하는데, 우선 대화가 복수의 페이지에 걸쳐있는지에 따라 판단 기준이 달라진다.   
   
복수 페이지에 걸쳐있지 않은 단순 액세스라면 웹 서버의 부하 상태가 판단 근거가 된다.   
웹 서버와 정기적으로 정보를 교환해 CPU나 메모리의 사용률을 수집하고, 이것을 바탕으로 부하가 낮은 웹 서버를 판단한다.   
혹은 시험 패킷을 웹 서버에 보내 응답 시간으로 부하를 판단하거나, 미리 웹 서버의 능력 비율에 따라 리퀘스트를 분배할 수도 있다.   
   
대화가 복수 페이지에 걸쳐있을 때는 웹 서버 부하에 관계 없이 이전 리퀘스트와 같은 웹 서버에 전송한다.   
그러나 HTTP 기본 동작은 리퀘스트 메시지를 보내기 전에 TCP 접속 동작을 하고, 응답 메시지를 반송하면 연결을 끊는다.    
즉, HTTP 상의 연결은 connectless해 stateless한 특징을 가지고 있으므로 대화가 복수 페이지에 걸쳐있는지 확인할 수 없다.   
이런 문제를 해결하기 위해 HTTP 헤더 필드에 전후 관계를 판단하기 위한 정보를 추가하는 방법(쿠키)을 사용한다.

## 4 캐시 서버를 이용한 서버의 부하 분산
    1. 캐시 서버의 이용   
여러 대 웹 서버를 설치하는 것이 아니라 다른 방법으로 부하 분산을 하는 방법도 있다.   
데이터베이스 서버와 웹 서버 같은 역할에 따라 서버를 나누는 방법으로, 캐시 서버를 사용하는 방법이다.    
   
캐시 서버는 ***프록시***라는 구조를 사용해 데이터를 캐시에 저장하는 서버다. 프록시는 웹 서버와 클라이언트 사이에 들어가 웹 서버에 대한 액세스 동작을 중개한다.   
이때 프록시는 웹 서버에서 받은 데이터를 디스크에 저장해 두고 웹 서버를 대신해 데이터를 클라이언트에 반송하는 기능을 가지고 있다.   
이것을 캐시라고 부른다. 웹 서버는 내부에서 여러 처리를 하기 때문에 시간이 걸리는 반면, 캐시 서버는 보존해 둔 데이터를 읽어 송신만 하므로 보다 빠르다.   
    
    2. 캐시 서버는 갱신일로 콘텐츠를 관리한다   
캐시 서버를 사용할 때는 부하 분산 장치와 마찬가지로 캐시 서버를 웹 서버 대신 DNS 서버에 등록한다.   
사용자가 캐시 서버에 HTTP 리퀘스트 메시지를 보내면 캐시 서버는 메시지의 내용을 조사하고, 데이터가 자신의 캐시에 저장되었는지 조사한다.   
   
1) 데이터가 캐시에 저장되어 않은 경우    
캐시 서버는 리퀘스트 메시지에 캐시 서버를 경유한 것을 나타내는 'Via'라는 헤더 필드를 추가해 웹 서버에 전송한다.   
만약 한 대의 캐시로 여러 대의 서버의 데이터를 캐시에 저장하는 경우 리퀘스트 메시지의 내용에 따라 몇 가지 방법으로 전송 대상의 웹 서버를 판단한다.    
대표적으로 리퀘스트 메시지의 URI에 쓰여있는 디렉토리를 보고 판단하는 방법이 있다.    
예) URI가 /dir1/이라는 디렉토리였다면 www1.github.com에 전송한다. URI가 /dir2/라는 디렉토리라면 www2.github.com에 전송한다.   
   
이때 전송 대상의 웹 서버에 대해 캐시 서버가 클라이언트가 되어 리퀘스트 메시지를 보낸다. 소켓을 만들고 웹 서버의 소켓에 접속해 송신하는 식이다.   
그러면 웹 서버에서 보내는 응답 메시지는 캐시 서버에 돌아온다.    
이번에는 클라이언트에 대해 웹 서버가 되어 'Via' 헤더 필드를 부가해 응답 메시지를 전송한다. 그 응답 메시지는 캐시에 저장하고 저장한 일시를 기록한다.   
   
이렇게 클라이언트와 웹 서버 사이를 중개하는 것이 프록시 구조이다. 중개할 때 페이지의 데이터를 저장하면 캐시 서버에 데이터가 축적된다.   
그러면 사용자가 액세스한 페이지가 캐시 서버에 저장되어 있는 경우가 증가한다.
   
2) 데이터가 캐시 서버에 저장되어 있는 경우   
먼저 사용자로부터 도착한 리퀘스트 메시지를 받아 캐시에 저장되었는지 조사한다.    
웹 서버측에서 데이터가 변경되었는지 조사하기 위해 'If-Modified-Since'라는 헤더 필드를 추가해 웹 서버에 전송한다.   
웹 서버는 'If-Modified-Since' 헤더 필드의 값과 페이지 데이터의 최종 갱신 일시를 비교해 변경이 없으면 변경이 없다는 응답 메시지를 반송한다.   
   
응답 메시지가 캐시 서버에 도착하면 캐시에 저장한 데이터가 최신 데이터와 같은지 알 수 있다.   
같다면 캐시 서버는 캐시에서 데이터를 추출해 사용자에게 보낸다.   
웹 서버측에서 데이터가 변경된 경우에는 캐시에 데이터가 저장되어 있지 않은 경우와 같이 동작한다.   
웹 서버는 최신 데이터를 반송하므로 메시지에 'Via' 헤더를 부가해 사용자에게 전송하고 데이터를 캐시에 저장한다.   

    3. 프록시의 원점은 포워드 프록시이다   
지금까지는 프록시라는 구조를 웹 서버측에 두고 캐시 기능을 사용한 것이었지만, 클라이언트측에 캐시 서버를 두는 방법도 있다.   
캐시 서버에서 이용하는 프록시라는 구조는 원래 클라이언트측에 두는 방법에서 시작했다. 이것이 프록시의 원형으로, ***포워드 프록시***라고 한다.   
포워드 프록시는 애초에 캐시를 이용하는 것에 추가적으로, 방화벽을 실현한다는 목적이 하나 더 있었다.   
   
우선 클라이언트측에 프록시 구조를 둘 때, 사내 LAN에서 프록시에 액세스하기만 하면 데이터를 손에 넣을 수 있으므로 매우 빨라진다.   
또 프록시는 리퀘스트의 내용을 조사한 후 전송하므로 리퀘스트 내용에 따라 액세스가 가능한지 판단이 가능하다.   
패킷 필터링형 방화벽보다 보다 자세히 조건을 설정하는 것이 가능한 것이다.   
   
포워드 프록시를 사용할 경우 보통 브라우저의 설정 화면에 준비되어 있는 프록시 서버라는 항목에 포워드 프록시의 IP 주소를 설정한다.    
그러면 브라우저의 리퀘스트 메시지 송신 동작과 리퀘스트 메시지가 달라진다.    
   
    1) 포워드 프록시를 설정하지 않으면    
    브라우저는 URL 입력 상자에 입력된 http://... 문자열에서 액세스 대상 웹 서버를 계산하고, 여기에 리퀘스트를 보낸다.   
    URL에서 웹 서버의 이름을 제외하고 파일이나 프로그램 경로명의 일부를 추출하여 이것을 리퀘스트의 URI 부분에 기록한다.   
    메시지를 전송할 때는 서버측에 둔 캐시 서버에 설정해 둔 웹 서버에만 전송할 수 있다.   
   
    2) 포워드 프록시를 설정하면    
    URL에 상관 없이 리퀘스트를 전부 포워드 프록시에 보낸다. 리퀘스트의 URI에는 http://... 라는 URL 그대로 기록한다.   
    메시지를 전송할 때는 URI 부분에 URL이 그대로 쓰여있으므로 URL이 전송 대상이 된다.   
    그러므로 서버측에 두는 캐시 서버와 같이 전송 대상 웹 서버를 사전에 설정해 둘 필요가 없고, 모든 웹 서버에서 전송할 수 있다.   

    4. 포워드 프록시를 개량한 리버스 프록시   
포워드 프록시를 사용할 경우 브라우저에 대한 설정이 꼭 필요하다는 것이 포워드 프록시의 특징이다.   
그런데 이 방법은 브라우저의 설정이 번거롭고 잘못 설정할 가능성이 있어 장애의 원인이 되기도 한다.   
또한 웹 서버는 누가 액세스하는지 알 수 없고, 브라우저에 프록시를 설정할 수 없기 때문에 포워드 프록시를 잘 쓰지 않는다.   
    
이에 브라우저에 프록시를 설정하지 않아도 사용할 수 있도록 개량되었다.   
리퀘스트 메시지의 URI에 쓰여있는 디렉토리명과 전송 대상의 웹 서버를 대응시켜    
URI 부분에 URL 전체가 쓰여있지 않은 보통의 리퀘스트 메시지를 전송할 수 있도록 했다.   
이것이 서버측에 설치하는 캐시 서버에 채택하고 있는 방식인 ***리버스 프록시***이다.

    5. 트랜스페어런트 프록시   
캐시 서버에서 전송 대상을 판단하는 방법, 즉 리퀘스트 메시지에서 패킷의 헤더를 조사하는 방법이 있다.   
패킷의 맨 앞 IP 헤더에는 수신처 IP 주소가 기록돼 있으므로 이것을 조사하면 액세스 대상 웹 서버가 어디 있는지 알 수 있는데,   
이 방법을 트랜스페이런트 프록시라고 부른다.   
   
트랜스페어런트 프록시는 포워드 프록시와 리버스 프록시의 장점만 모은 구조이지만, 여기에 리퀘스트 메시지를 건네주는 방법을 주의해야 한다.   
트랜스페어런트 프록시는 브라우저에 설정하지 않으므로 브라우저는 웹 서버에 리퀘스트 메시지를 보낸다.   
리버스 프록시와 같이 DNS 서버에 등록하는 방법이라면 이 리퀘스트 메시지가 프록시에 도착하지만, 트랜스페어런트 프록시는 DNS 서버에 등록하지 않는다.   
   
DNS 서버에 등록하면 트랜스페어런트 프록시 자체가 액세스 대상이 되어 수신처 IP 주소로 전송 대상의 웹 서버를 판단한다는 구조를 이용할 수 없게 된다.   
그러면 리퀘스트 메시지는 브라우저에서 서버로 흘러갈 뿐 트랜스페어런트 프록시에는 도착하지 않는 것이다.   
   
이것을 해결하기 위해 브라우저에서 서버로 리퀘스트 메시지가 흘러가는 길에 트랜스페어런트 프록시를 설치한다.   
그리고 메시지가 트랜스페어런트 프록시를 통과할 때 그것을 가로챈다.    
리퀘스트 메시지가 흐르는 길이 많으면 여기에 전부 트랜스페어런트 프록시를 설치해야 하므로 길이 한 개로 수렴하는 형태로 네트워크를 만들어   
수렴되는 곳에 트랜스페어런트 프록시를 설치하는 것이 보통이다.   
   
트랜스페어런트 프록시를 사용하면 사용자가 프록시의 존재를 알아차릴 필요가 거의 없어진다.   
따라서 HTTP의 메시지를 전송한다는 구조에 대한 관심이 적어지고 캐시를 이용한다는 측면에서 비중이 높아지고 있다.   
요즘은 프록시라고 부르지 않고 캐시라고 부르는 예가 늘어나고 있다.

## 5 콘텐츠 배포 서비스
    1. 콘텐츠 배포 서비스(Content Delivery Service)를 이용한 부하 분산    
캐시 서버는 서버측에 두는 경우와 클라이언트측에 두는 경우가 이용 효과 면에서 차이가 난다.   
서버측에 캐시 서버를 두는 방법은 웹 서버의 부하를 경감하는 효과는 있지만, 인터넷 트래픽을 억제하는 효과는 없다.   
반면, 클라이언트측에 캐시 서버가 있으면 이러한 혼잡에 휘말려드는 일이 없으므로 패킷 흐름이 안정된다.   
대신 클라이언트측이 캐시 서버를 관리하므로 웹 서버 운영자는 캐시 서버를 제어할 수 없다.   
   
캐시 서버는 놓는 장소에 따라 장단점이 있는데, 양쪽의 좋은 점을 취한 방법은   
프로바이더와 계약해 웹 서버 운영자가 제어할 수 있는 캐시 서버를 클라이언트측 프로바이더(인터넷 통신사)에 두는 것이다.   
이 방법의 문제점은 인터넷에 공개하는 서버는 인터넷의 어디에서 액세스하는지 모른다는 점이다.   
따라서 프로바이더의 POP(Point Of Presence, 통신사용 대형 라우터) 전부에 캐시 서버를 설치해야 하지만, 수가 너무 많아 비현실적이다.   
   
이 문제는 중요한 프로바이더에 중점을 두면 캐시 서버 수를 줄일 수 있다.    
그러나 이 방법도 웹 서버 운영자가 스스로 프로바이더와 계약해 캐시 서버를 설치한다기에 비용, 노력면에서 상당히 힘들어진다.   
이것을 해결하는 방법이 캐시 서버를 설치하고 웹 서버 운영자에게 대출하는 서비스인 ***콘텐츠 배포 서비스***이다.   
   
CDN(Content Delivery/Distribution Network)으로도 잘 알려져 있고, AWS Cloud Front가 대표 업체이다.
이 서비스를 제공하는 사업자 CDSP(Content Delivery Service Provider)는 중요한 프로바이더와 계약하고 그곳에 다수의 캐시 서버를 설치한다.   
CDSP는 웹 서버와도 계약해 서버와 CDSP의 캐시 서버를 연대시킨다. 그러면 클라이언트가 웹 서버에 액세스할 때 CDSP의 캐시 서버에 액세스하게 된다.   
   
<img width='500px' src='https://user-images.githubusercontent.com/31424628/145520412-d6c0a00a-8956-471f-943f-0703303107c5.png' />

    2. 가장 가까운 캐시 서버의 관점   
콘텐츠 배포 서비스(CDS)를 사용하는 경우 인터넷 전체에 설치된 다수의 캐시 서버를 이용한다.    
다수의 캐시 서버 중 가장 가까운 캐시 서버를 찾아내고, 클라이언트는 여기에 액세스하도록 중재하는 구조가 필요하다.   
이 방법에는 몇 가지가 있는데, 최초의 방법은 복수 서버에서 부하를 분산시킬 때 DNS 서버에서 액세스를 분배하는 것과 비슷하다.   
   
DNS 서버는 인터넷에 다수 배치되어 있고, 이것들이 연대해 조회와 회답을 하는 프로세스는 다음과 같다   
1) 최초에 액세스 대상 웹 서버를 기록한 조회 메시지를 만들어 자신의 LAN에 있는 DNS 서버에 보낸다.   
2) 그러면 클라이언트측 DNS 서버는 웹 서버 이름의 계층 구조를 조사해 이름이 등록되어 있는 DNS 서버에 조회 메시지를 보낸다.   
3) 웹 서버측 DNS 서버는 서버명과 IP 주소를 대응시킨 대응표를 보고 이름에 대응하는 IP 주소를 조사해 회답한다.   
4) 회답이 클라이언트 측 DNS 서버에 도착하고, 클라이언트측에 회답이 되돌려진다.   
   
만약 한 개의 이름에 복수의 IP 주소를 대응시킨 경우에는 라운드 로빈에 의해 차례로 IP 주소를 회답한다.   
여기까지가 보통 DNS 서버의 동작이다. 이것을 그대로 사용하면 라운드 로빈을 이용해 차례대로 IP 주소를 회답한다.   
그러나 먼 위치에 있는 캐시 서버의 IP 주소를 돌려줄 수도 있기에 라운드 로빈을 사용하면 안된다.   
   
클라이언트와 캐시 서버의 거리를 판단해 가장 가까운 캐시 서버의 IP 주소를 회답하는 방법은 다음과 같다.   
    1) 캐시 서버의 설치 장소에 있는 라우터에서 경로 정보를 모아둔 경로표를 입수해 DNS 서버 곁에 모은다.    
    2) 경로표를 사용해 DNS 조회 메시지의 송신처, 즉 클라이언트측의 DNS 서버에 이르는 경로 정보를 조사한다.   
    3) 각 라우터 별로 대략적인 거리를 알 수 있고, 어느 라우터가 클라이언트측 DNS 서버와 가장 가까운지 파악한다.   

    3. 리피터용 서버로 액세스 대상을 분배한다   
가장 가까운 캐시 서버에 액세스하는 또다른 방법은 HTTP 헤더 필드의 'Location'이라는 헤더를 이용하는 것이다.   
이것은 웹 서버의 데이터를 다른 서버로 옮기는 경우 사용하는데, '그 데이터는 여기 있으니 그쪽으로 다시 액세스하세요.' 라는 의미다.   
이렇게 다른 웹 서버에 액세스하도록 처리하는 것을 ***리다이렉트(redirect)*** 라고 한다.   
이것을 사용해 액세스 대상을 가장 가까운 캐시 서버로 돌리는 것이다.   
   
우선 리다이렉트용 서버를 웹 서버측 DNS 서버에 등록한다. 클라이언트는 여기에 HTTP 리퀘스트 메시지를 보내게 된다.   
리다이렉트용 서버에는 DNS 서버와 같이 라우터에서 모은 경로 정보가 있으며, 여기에서 가장 가까운 캐시 서버를 찾는다.   
캐시 서버를 나타내는 Location 헤더를 붙여 응답하면 클라이언트는 캐시 서버에 다시 액세스하게 되는 것이다.   
    
이 방법은 리다이렉트의 HTTP 메시지 대화가 증가하므로 그만큼 오버헤드가 많지만, 장점도 있다.   
DNS 서버를 세밀히 계산하는 방법은 클라이언트측 DNS 서버와 캐시 서버의 거리를 계산하므로 정밀도가 떨어질 수 있다.   
그러나 리다이렉트는 클라이언트가 보내는 HTTP 메시지의 송신처 IP 주소를 바탕으로 거리를 판단하므로 정밀도가 높다.   
    
경로 정보를 사용하지 않고 다른 정보에서 거리를 계산해 정밀도를 높일 수도 있다.   
리다이렉트용 서버가 클라이언트에 반송하는 것은 Location 헤더를 포함한 HTTP 메시지뿐만이 아니다.   
   
패킷의 왕복 시간을 통해 캐시 서버까지의 거리를 계산해 최적의 캐시 서버에 액세스하도록 스크립트 프로그램을 내장한 페이지를 반송하는 방법도 있다.   
이 페이지에는 몇 개의 캐시 서버에 시험적으로 패킷을 보내 왕복 시간을 계측한 후 가장 짧은 캐시 서버에 리퀘스트를 다시 보낸다.   
이렇게 클라이언트 스스로 최적의 캐시 서버를 판단하고, 여기에 액세스하는 것이다.   

    4. 캐시 내용의 갱신 방법에서 성능의 차이가 난다
캐시 서버의 효율을 좌우하는 요소 중 캐시의 내용을 갱신하는 방법이 있다.   
캐시의 개념 상 데이터가 저장되어 있지 않을 때 최초의 액세스 동작에는 캐시가 도움이 되지 않는다.   
또한 두 번째 이후 액세스에서도 원래 데이터를 가진 웹 서버에 갱신 유무를 확인하기에 이것이 혼잡하게 얽히면 응답 시간이 악화된다.   
   
이런 점을 개선하기 위해 웹 서버에서 원래 데이터를 갱신할 경우 이것을 즉시 캐시 서버에 반영해야 한다.   
그러면 데이터의 갱신 유무를 확인할 필요가 없고, 최초의 액세스 때도 캐시 데이터를 이용할 수 있게 된다.   
콘텐츠 배포 서비스에 이용하는 캐시 서버에는 이런 대책이 내장되어 있다.   
   
웹 페이지는 CGI 앱 등 동적으로 페이지를 만드는 경우가 있는데, 이 경우에는 캐시 서버에 데이터를 저장해 두면 안된다.   
이 경우 매번 달라지는 부분과 달라지지 않는 부분을 구분해 변하지 않는 부분만 캐시에 저장해야 한다.
